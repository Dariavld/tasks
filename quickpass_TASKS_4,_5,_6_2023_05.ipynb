{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "81ee93a1-8801-48e5-a4ec-4c4ac8c34fea",
      "metadata": {
        "tags": [],
        "id": "81ee93a1-8801-48e5-a4ec-4c4ac8c34fea"
      },
      "source": [
        "### Лингвистическая часть\n",
        "\n",
        "Для выполнения этих заданий выберите два любых достаточно длинных текста (.txt) на русском и на любом другом (для которого есть парсеры) языке; если возьмете текст и его перевод, будет отлично."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c5051a4-23d6-4e51-9fa1-044f183ae04f",
      "metadata": {
        "id": "4c5051a4-23d6-4e51-9fa1-044f183ae04f"
      },
      "source": [
        "#### Задача 4. \n",
        "\n",
        "Просмотрите оба выбранных текста. Удостоверьтесь, что тексты чистые, если же в них есть какой-то мусор: хештеги, затесавшиеся при OCR символы и подобное, почистите с помощью регулярных выражений. \n",
        "\n",
        "Проведите первичный статистический анализ: разбейте тексты на предложения и на токены, посчитайте относительное количество того и другого, сопоставьте. Если ваши тексты параллельные, какой длиннее? В каком тексте средняя длина предложения больше? Почему? В каком тексте выше лексическое разнообразие? \n",
        "\n",
        "Таким образом, вам необходимо узнать следующие вещи:\n",
        "\n",
        "- количество предложений (относительное и абсолютное)\n",
        "- количество токенов (относительное и абсолютное)\n",
        "- средняя длина предложения (среднее количество слов в предложении)\n",
        "- соотношение \"уникальные токены / все токены\"\n",
        "- (опционально) соотношение знаков пунктуации и слов"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prus = \"/content/portrait_rus.txt\"\n",
        "prus = open(prus, \"r\")\n",
        "prus = prus.read()\n",
        "prus\n",
        "\n",
        "peng = \"/content/portrait_eng.txt\"\n",
        "peng = open(peng, \"r\")\n",
        "peng = peng.read()\n",
        "peng"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "kmnC_u1QmUG2",
        "outputId": "df8b67a5-2316-4fdc-c5be-9da07af27359"
      },
      "id": "kmnC_u1QmUG2",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The studio was filled with the rich odour of roses, and when the light summer wind stirred amidst the trees of the garden, there came through the open door the heavy scent of the lilac, or the more delicate perfume of the pink-flowering thorn.\\n\\nFrom the corner of the divan of Persian saddle-bags on which he was lying, smoking, as was his custom, innumerable cigarettes, Lord Henry Wotton could just catch the gleam of the honey-sweet and honey-coloured blossoms of a laburnum, whose tremulous branches seemed hardly able to bear the burden of a beauty so flamelike as theirs; and now and then the fantastic shadows of birds in flight flitted across the long tussore-silk curtains that were stretched in front of the huge window, producing a kind of momentary Japanese effect, and making him think of those pallid, jade-faced painters of Tokyo who, through the medium of an art that is necessarily immobile, seek to convey the sense of swiftness and motion. The sullen murmur of the bees shouldering their way through the long unmown grass, or circling with monotonous insistence round the dusty gilt horns of the straggling woodbine, seemed to make the stillness more oppressive. The dim roar of London was like the bourdon note of a distant organ.\\n\\nIn the centre of the room, clamped to an upright easel, stood the full-length portrait of a young man of extraordinary personal beauty, and in front of it, some little distance away, was sitting the artist himself, Basil Hallward, whose sudden disappearance some years ago caused, at the time, such public excitement and gave rise to so many strange conjectures.\\n\\nAs the painter looked at the gracious and comely form he had so skilfully mirrored in his art, a smile of pleasure passed across his face, and seemed about to linger there. But he suddenly started up, and closing his eyes, placed his fingers upon the lids, as though he sought to imprison within his brain some curious dream from which he feared he might awake.\\n\\n“It is your best work, Basil, the best thing you have ever done,” said Lord Henry languidly. “You must certainly send it next year to the Grosvenor. The Academy is too large and too vulgar. Whenever I have gone there, there have been either so many people that I have not been able to see the pictures, which was dreadful, or so many pictures that I have not been able to see the people, which was worse. The Grosvenor is really the only place.”\\n\\n“I don’t think I shall send it anywhere,” he answered, tossing his head back in that odd way that used to make his friends laugh at him at Oxford. “No, I won’t send it anywhere.”\\n\\nLord Henry elevated his eyebrows and looked at him in amazement through the thin blue wreaths of smoke that curled up in such fanciful whorls from his heavy, opium-tainted cigarette. “Not send it anywhere? My dear fellow, why? Have you any reason? What odd chaps you painters are! You do anything in the world to gain a reputation. As soon as you have one, you seem to want to throw it away. It is silly of you, for there is only one thing in the world worse than being talked about, and that is not being talked about. A portrait like this would set you far above all the young men in England, and make the old men quite jealous, if old men are ever capable of any emotion.”\\n\\n“I know you will laugh at me,” he replied, “but I really can’t exhibit it. I have put too much of myself into it.”\\n\\nLord Henry stretched himself out on the divan and laughed.\\n\\n“Yes, I knew you would; but it is quite true, all the same.”\\n\\n“Too much of yourself in it! Upon my word, Basil, I didn’t know you were so vain; and I really can’t see any resemblance between you, with your rugged strong face and your coal-black hair, and this young Adonis, who looks as if he was made out of ivory and rose-leaves. Why, my dear Basil, he is a Narcissus, and you—well, of course you have an intellectual expression and all that. But beauty, real beauty, ends where an intellectual expression begins. Intellect is in itself a mode of exaggeration, and destroys the harmony of any face. The moment one sits down to think, one becomes all nose, or all forehead, or something horrid. Look at the successful men in any of the learned professions. How perfectly hideous they are! Except, of course, in the Church. But then in the Church they don’t think. A bishop keeps on saying at the age of eighty what he was told to say when he was a boy of eighteen, and as a natural consequence he always looks absolutely delightful. Your mysterious young friend, whose name you have never told me, but whose picture really fascinates me, never thinks. I feel quite sure of that. He is some brainless beautiful creature who should be always here in winter when we have no flowers to look at, and always here in summer when we want something to chill our intelligence. Don’t flatter yourself, Basil: you are not in the least like him.”'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vkd2VMcEtufQ",
        "outputId": "6c15e6d7-697b-4ad2-c308-601ab42b6cb2"
      },
      "id": "Vkd2VMcEtufQ",
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "peng_tok_sent = sent_tokenize(peng)\n",
        "peng_tok_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUz4Jr8q5GGZ",
        "outputId": "a3e67220-4c12-48b1-b0a8-48b23bb6daa4"
      },
      "id": "IUz4Jr8q5GGZ",
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The studio was filled with the rich odour of roses and when the light summer wind stirred amidst the trees of the garden there came through the open door the heavy scent of the lilac or the more delicate perfume of the pinkflowering thorn From the corner of the divan of Persian saddlebags on which he was lying smoking as was his custom innumerable cigarettes Lord Henry Wotton could just catch the gleam of the honeysweet and honeycoloured blossoms of a laburnum whose tremulous branches seemed hardly able to bear the burden of a beauty so flamelike as theirs and now and then the fantastic shadows of birds in flight flitted across the long tussoresilk curtains that were stretched in front of the huge window producing a kind of momentary Japanese effect and making him think of those pallid jadefaced painters of Tokyo who through the medium of an art that is necessarily immobile seek to convey the sense of swiftness and motion The sullen murmur of the bees shouldering their way through the long unmown grass or circling with monotonous insistence round the dusty gilt horns of the straggling woodbine seemed to make the stillness more oppressive The dim roar of London was like the bourdon note of a distant organ In the centre of the room clamped to an upright easel stood the fulllength portrait of a young man of extraordinary personal beauty and in front of it some little distance away was sitting the artist himself Basil Hallward whose sudden disappearance some years ago caused at the time such public excitement and gave rise to so many strange conjectures As the painter looked at the gracious and comely form he had so skilfully mirrored in his art a smile of pleasure passed across his face and seemed about to linger there But he suddenly started up and closing his eyes placed his fingers upon the lids as though he sought to imprison within his brain some curious dream from which he feared he might awake It is your best work Basil the best thing you have ever done said Lord Henry languidly You must certainly send it next year to the Grosvenor The Academy is too large and too vulgar Whenever I have gone there there have been either so many people that I have not been able to see the pictures which was dreadful or so many pictures that I have not been able to see the people which was worse The Grosvenor is really the only place I dont think I shall send it anywhere he answered tossing his head back in that odd way that used to make his friends laugh at him at Oxford No I wont send it anywhere Lord Henry elevated his eyebrows and looked at him in amazement through the thin blue wreaths of smoke that curled up in such fanciful whorls from his heavy opiumtainted cigarette Not send it anywhere My dear fellow why Have you any reason What odd chaps you painters are You do anything in the world to gain a reputation As soon as you have one you seem to want to throw it away It is silly of you for there is only one thing in the world worse than being talked about and that is not being talked about A portrait like this would set you far above all the young men in England and make the old men quite jealous if old men are ever capable of any emotion I know you will laugh at me he replied but I really cant exhibit it I have put too much of myself into it Lord Henry stretched himself out on the divan and laughed Yes I knew you would but it is quite true all the same Too much of yourself in it Upon my word Basil I didnt know you were so vain and I really cant see any resemblance between you with your rugged strong face and your coalblack hair and this young Adonis who looks as if he was made out of ivory and roseleaves Why my dear Basil he is a Narcissus and you well of course you have an intellectual expression and all that But beauty real beauty ends where an intellectual expression begins Intellect is in itself a mode of exaggeration and destroys the harmony of any face The moment one sits down to think one becomes all nose or all forehead or something horrid Look at the successful men in any of the learned professions How perfectly hideous they are Except of course in the Church But then in the Church they dont think A bishop keeps on saying at the age of eighty what he was told to say when he was a boy of eighteen and as a natural consequence he always looks absolutely delightful Your mysterious young friend whose name you have never told me but whose picture really fascinates me never thinks I feel quite sure of that He is some brainless beautiful creature who should be always here in winter when we have no flowers to look at and always here in summer when we want something to chill our intelligence Dont flatter yourself Basil you are not in the least like him']"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "20b00fbd-f67e-40ca-81c4-bce07b4b3103",
      "metadata": {
        "id": "20b00fbd-f67e-40ca-81c4-bce07b4b3103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb50fe5e-8550-4c87-dea2-409f892a5842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rus_num_sentences': 43, 'eng_num_sentences': 37, 'rus_num_tokens': 947, 'eng_num_tokens': 1026, 'rus_rel_num_sentences': 0.5375, 'eng_rel_num_sentences': 0.4625, 'rus_rel_num_tokens': 0.4799797263051191, 'eng_rel_num_tokens': 0.5200202736948809, 'rus_avg_sentence_length': 22.023255813953487, 'eng_avg_sentence_length': 27.72972972972973, 'rus_ratio_unique_tokens': 0.5533262935586061, 'eng_ratio_unique_tokens': 0.4337231968810916, 'rus_ratio_punctuation_marks': 0.18373812038014783, 'eng_ratio_punctuation_marks': 0.1111111111111111}\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize.punkt import REASON_DEFAULT_DECISION\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def primary_statistical_analysis(prus, peng):\n",
        "    try:\n",
        "  \n",
        "        rus_sentences = sent_tokenize(prus)\n",
        "        eng_sentences = sent_tokenize(peng)\n",
        "        rus_words = word_tokenize(prus)\n",
        "        eng_words = word_tokenize(peng)\n",
        "        \n",
        "        rus_num_sentences = len(rus_sentences)\n",
        "        eng_num_sentences = len(eng_sentences)\n",
        "        rus_num_tokens = len(rus_words)\n",
        "        eng_num_tokens = len(eng_words)\n",
        "        \n",
        "        rus_rel_num_sentences = rus_num_sentences / (rus_num_sentences + eng_num_sentences)\n",
        "        eng_rel_num_sentences = eng_num_sentences / (rus_num_sentences + eng_num_sentences)\n",
        "        rus_rel_num_tokens = rus_num_tokens / (rus_num_tokens + eng_num_tokens)\n",
        "        eng_rel_num_tokens = eng_num_tokens / (rus_num_tokens + eng_num_tokens)\n",
        "        \n",
        "        rus_avg_sentence_length = rus_num_tokens / rus_num_sentences\n",
        "        eng_avg_sentence_length = eng_num_tokens / eng_num_sentences\n",
        "        \n",
        "        rus_unique_tokens = set(rus_words)\n",
        "        eng_unique_tokens = set(eng_words)\n",
        "        rus_ratio_unique_tokens = len(rus_unique_tokens) / rus_num_tokens\n",
        "        eng_ratio_unique_tokens = len(eng_unique_tokens) / eng_num_tokens\n",
        "        \n",
        "        rus_punctuation_marks = [word for word in rus_words if word in string.punctuation]\n",
        "        eng_punctuation_marks = [word for word in eng_words if word in string.punctuation]\n",
        "        rus_ratio_punctuation_marks = len(rus_punctuation_marks) / rus_num_tokens\n",
        "        eng_ratio_punctuation_marks = len(eng_punctuation_marks) / eng_num_tokens\n",
        "        \n",
        "        results = {\n",
        "            \"rus_num_sentences\": rus_num_sentences,\n",
        "            \"eng_num_sentences\": eng_num_sentences,\n",
        "            \"rus_num_tokens\": rus_num_tokens,\n",
        "            \"eng_num_tokens\": eng_num_tokens,\n",
        "            \"rus_rel_num_sentences\": rus_rel_num_sentences,\n",
        "            \"eng_rel_num_sentences\": eng_rel_num_sentences,\n",
        "            \"rus_rel_num_tokens\": rus_rel_num_tokens,\n",
        "            \"eng_rel_num_tokens\": eng_rel_num_tokens,\n",
        "            \"rus_avg_sentence_length\": rus_avg_sentence_length,\n",
        "            \"eng_avg_sentence_length\": eng_avg_sentence_length,\n",
        "            \"rus_ratio_unique_tokens\": rus_ratio_unique_tokens,\n",
        "            \"eng_ratio_unique_tokens\": eng_ratio_unique_tokens,\n",
        "            \"rus_ratio_punctuation_marks\": rus_ratio_punctuation_marks,\n",
        "            \"eng_ratio_punctuation_marks\": eng_ratio_punctuation_marks\n",
        "        }\n",
        "        \n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "results_for_texts = primary_statistical_analysis(prus, peng)\n",
        "\n",
        "\n",
        "print(results_for_texts)          "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28d22ca8-4de5-4dac-a9c0-f78bf4b6cf39",
      "metadata": {
        "id": "28d22ca8-4de5-4dac-a9c0-f78bf4b6cf39"
      },
      "source": [
        "#### Задача 5. \n",
        "\n",
        "Сделайте морфосинтаксические разборы ваших текстов в формате UD, запишите .conllu-файлы. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy_udpipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i8S2_rVBUE-",
        "outputId": "6ea54ba4-44d4-4440-9afe-5246f4f6fbba"
      },
      "id": "-i8S2_rVBUE-",
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy_udpipe\n",
            "  Downloading spacy_udpipe-1.0.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy_udpipe) (3.5.2)\n",
            "Collecting ufal.udpipe>=1.2.0 (from spacy_udpipe)\n",
            "  Downloading ufal.udpipe-1.3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (936 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m937.0/937.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.1.2)\n",
            "Installing collected packages: ufal.udpipe, spacy_udpipe\n",
            "Successfully installed spacy_udpipe-1.0.0 ufal.udpipe-1.3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "ef443021-3c8f-45ad-a102-b9740a0daa85",
      "metadata": {
        "id": "ef443021-3c8f-45ad-a102-b9740a0daa85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348aae58-6169-479e-e995-eaeacf51a84f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: corpy in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from corpy) (2022.10.31)\n",
            "Requirement already satisfied: lxml>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from corpy) (4.9.2)\n",
            "Requirement already satisfied: wordcloud>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from corpy) (1.8.2.2)\n",
            "Requirement already satisfied: ufal.morphodita>=1.10 in /usr/local/lib/python3.10/dist-packages (from corpy) (1.11.1.1)\n",
            "Requirement already satisfied: ufal.udpipe>=1.2 in /usr/local/lib/python3.10/dist-packages (from corpy) (1.3.0.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from corpy) (1.22.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from corpy) (8.1.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud>=1.8.1->corpy) (8.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud>=1.8.1->corpy) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud>=1.8.1->corpy) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud>=1.8.1->corpy) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud>=1.8.1->corpy) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud>=1.8.1->corpy) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud>=1.8.1->corpy) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud>=1.8.1->corpy) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud>=1.8.1->corpy) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud>=1.8.1->corpy) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=63465ad61e1b53203f4dc314c2b3e0db2e18d8e0a841aa67a8ee25c69396bf93\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ],
      "source": [
        "!pip install corpy\n",
        "!pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy_udpipe\n",
        "spacy_udpipe.download('en') \n",
        "import pymorphy2\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "parse = morph.parse('skilfully')\n",
        "parse\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t1FIhZtB52n",
        "outputId": "2ce5bbd1-f86f-4989-9e83-5591c41776c9"
      },
      "id": "0t1FIhZtB52n",
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already downloaded a model for the 'en' language\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parse(word='skilfully', tag=OpencorporaTag('LATN'), normal_form='skilfully', score=1.0, methods_stack=((LatinAnalyzer(score=0.9), 'skilfully'),))]"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parse = morph.parse('написанных')\n",
        "t = parse[0].tag \n",
        "print(f'Часть речи: {t.POS}')\n",
        "print(f'Одушевленность: {t.animacy}\\nПадеж: {t.case}\\nРод: {t.gender}\\nНаклонение: {t.mood}\\\n",
        "\\nЧисло: {t.number}\\nЛицо: {t.person}\\nВремя: {t.tense}\\nПереходность: {t.transitivity}\\nЗалог: {t.voice}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWzwXuyiCFWz",
        "outputId": "3b6e9cf0-52ec-4bec-9abf-a5114beeaee8"
      },
      "id": "NWzwXuyiCFWz",
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Часть речи: PRTF\n",
            "Одушевленность: None\n",
            "Падеж: gent\n",
            "Род: None\n",
            "Наклонение: None\n",
            "Число: plur\n",
            "Лицо: None\n",
            "Время: past\n",
            "Переходность: tran\n",
            "Залог: pssv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b91d9717-183d-44c3-90d7-f06a996aae27",
      "metadata": {
        "id": "b91d9717-183d-44c3-90d7-f06a996aae27"
      },
      "source": [
        "#### Задача 6. \n",
        "\n",
        "Посчитайте статистику по частям речи, сопоставьте: можно напечатать две таблички с процентами по частям речи. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj0canZV_tKA",
        "outputId": "ea109cf5-cdfe-4b5f-97c1-dcc70e643149"
      },
      "id": "Dj0canZV_tKA",
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "def compare_parts_of_speech(prus, peng):\n",
        "    try:\n",
        "        tokens1 = word_tokenize(prus)\n",
        "        tokens2 = word_tokenize(peng)\n",
        "        \n",
        "        stop_words1 = set(stopwords.words('russian'))\n",
        "        stop_words2 = set(stopwords.words('english'))\n",
        "        filtered_tokens1 = [word for word in tokens1 if word.lower() not in stop_words1]\n",
        "        filtered_tokens2 = [word for word in tokens2 if word.lower() not in stop_words2]\n",
        "        \n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        lemmatized_tokens1 = [lemmatizer.lemmatize(word) for word in filtered_tokens1]\n",
        "        lemmatized_tokens2 = [lemmatizer.lemmatize(word) for word in filtered_tokens2]\n",
        "        \n",
        "        pos1 = pos_tag(lemmatized_tokens1)\n",
        "        pos2 = pos_tag(lemmatized_tokens2)\n",
        "        \n",
        "        freq_dist1 = FreqDist(tag for (word, tag) in pos1)\n",
        "        freq_dist2 = FreqDist(tag for (word, tag) in pos2)\n",
        "        \n",
        "        total_words1 = len(lemmatized_tokens1)\n",
        "        total_words2 = len(lemmatized_tokens2)\n",
        "        \n",
        "\n",
        "        print(\"Русский текст:\")\n",
        "        print(\"{:<10} {:<10} {:<10}\".format('POS', 'Count', 'Percentage'))\n",
        "        print(\"-\" * 30)\n",
        "        for tag, count in freq_dist1.items():\n",
        "            percentage = (count / total_words1) * 100\n",
        "            print(\"{:<10} {:<10} {:<10.2f}\".format(tag, count, percentage))\n",
        "        print(\"\\n\")\n",
        "        \n",
        "        print(\"Английский текст:\")\n",
        "        print(\"{:<10} {:<10} {:<10}\".format('POS', 'Count', 'Percentage'))\n",
        "        print(\"-\" * 30)\n",
        "        for tag, count in freq_dist2.items():\n",
        "            percentage = (count / total_words2) * 100\n",
        "            print(\"{:<10} {:<10} {:<10.2f}\".format(tag, count, percentage))\n",
        "        print(\"\\n\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        # Log the error\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "resu = compare_parts_of_speech(prus, peng)\n",
        "\n",
        "\n",
        "print(resu)   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld0e7yJS-soC",
        "outputId": "133dc293-acd2-4ec5-cd3c-db0ba342e6e2"
      },
      "id": "Ld0e7yJS-soC",
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Русский текст:\n",
            "POS        Count      Percentage\n",
            "------------------------------\n",
            "JJ         40         5.86      \n",
            "NNP        379        55.49     \n",
            ",          118        17.28     \n",
            ".          43         6.30      \n",
            "VB         18         2.64      \n",
            "NN         35         5.12      \n",
            ":          7          1.02      \n",
            "CC         4          0.59      \n",
            "(          2          0.29      \n",
            ")          2          0.29      \n",
            "FW         30         4.39      \n",
            "CD         2          0.29      \n",
            "VBD        1          0.15      \n",
            "UH         2          0.29      \n",
            "\n",
            "\n",
            "Английский текст:\n",
            "POS        Count      Percentage\n",
            "------------------------------\n",
            "NN         150        26.64     \n",
            "VBD        30         5.33      \n",
            "JJ         102        18.12     \n",
            ",          73         12.97     \n",
            ".          37         6.57      \n",
            "NNP        41         7.28      \n",
            "MD         6          1.07      \n",
            "VB         12         2.13      \n",
            "WP$        4          0.71      \n",
            "RB         41         7.28      \n",
            ":          4          0.71      \n",
            "VBN        9          1.60      \n",
            "IN         9          1.60      \n",
            "VBG        9          1.60      \n",
            "VBP        15         2.66      \n",
            "JJS        3          0.53      \n",
            "NNS        7          1.24      \n",
            "JJR        1          0.18      \n",
            "VBZ        4          0.71      \n",
            "CD         4          0.71      \n",
            "UH         1          0.18      \n",
            "RBR        1          0.18      \n",
            "\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5232e3cb-2ad7-4bc8-a541-e7a3fb6b678c",
      "metadata": {
        "id": "5232e3cb-2ad7-4bc8-a541-e7a3fb6b678c"
      },
      "source": [
        "#### Задача 7. \n",
        "\n",
        "Посчитайте, какое соотношение токенов по частям речи имеет совпадающие со словоформой леммы (т.е., в скольких случаях токены с частью речи VERB, например, имели словарную форму: и сам токен, и лемма одинаковые). Что вы можете сказать о выбранных вами языках на основании этих данных? Ожидаются две таблички с процентами несовпадающих по лемме и токену слов для каждой части речи. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5ba5e91-de7c-4043-9e4b-9c77c7a78748",
      "metadata": {
        "id": "d5ba5e91-de7c-4043-9e4b-9c77c7a78748"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a092bf73-5c80-45a5-b9ea-7963a38df24e",
      "metadata": {
        "id": "a092bf73-5c80-45a5-b9ea-7963a38df24e"
      },
      "source": [
        "#### Задача 8. \n",
        "\n",
        "Посчитайте медианную длину предложения для ваших текстов (медиана - это если взять все длины всех ваших предложений, упорядочить их от маленького к большому и выбрать то число, которое оказалось посередине, а если чисел четное количество, то взять среднее арифметическое двух чисел посередине. Например, если у вас пять предложений длинами 1, 2, 6, 7, 8, то медиана - 6, а если шесть предложений длинами 1, 1, 7, 9, 10, 11, то медиана - (7 + 9) / 2 = 8). Возьмите любые два предложения (одно русское и второе на другом языке) и постройте для них деревья зависимостей. Изучите связи зависимостей (deprel) и вершины: согласны ли вы с разбором?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a386e641-f1c1-4f9f-918c-5f3cc1efd080",
      "metadata": {
        "id": "a386e641-f1c1-4f9f-918c-5f3cc1efd080"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "244e1e1e-ff95-4c1e-8c3e-e74d0783da8c",
      "metadata": {
        "id": "244e1e1e-ff95-4c1e-8c3e-e74d0783da8c"
      },
      "source": [
        "#### Задача 9. \n",
        "\n",
        "Посчитайте частотные списки токенов для каждой категории связей зависимостей (т.е., нужно выделить все токены в тексте, которые получали, например, ярлык amod, и посчитать их частоты). Выведите по первые три самых частотных токена для каждой категории (punct можно не выводить). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ddcd036-6ad7-48f9-8bbe-ec276f46435a",
      "metadata": {
        "id": "5ddcd036-6ad7-48f9-8bbe-ec276f46435a"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1be38b9d-cb10-4cff-8942-ba3022448ba7",
      "metadata": {
        "id": "1be38b9d-cb10-4cff-8942-ba3022448ba7"
      },
      "source": [
        "#### Задача 10. \n",
        "\n",
        "Некоторые предлоги в русском языке могут управлять разными падежами (например, \"я еду в Лондон\" vs \"я живу в Лондоне\"). Давайте проанализируем эти предлоги и их падежи. Необходимо:\n",
        "\n",
        "- составить список таких предлогов (РГ-80 вам в помощь)\n",
        "- взять достаточно большой текст (можно большое художественное произведение)\n",
        "- сделать морфоразбор этого текста (лучше не pymorphy)\n",
        "- Посчитать, как часто и какие падежи встречаются у слова, идущего после предлога.\n",
        "\n",
        "Примечания: во-первых, имейте в виду, что иногда после предлога могут идти самые неожиданные вещи: \"я что, должен ехать на, черт побери, северный полюс?\". Во-вторых, неплохо бы учитывать отсутствие пунктуации (конечно, в норме, как нам кажется, предлог обязательно требует зависимое, но! \"да иди ты на!\") Эти штуки можно отсеять, если просто учитывать только заранее определенные падежи, а не считать все, какие встретились (так и None можно огрести).\n",
        "\n",
        "Если будете использовать RNNMorph, возможно, понадобится регулярное выражение и немного терпения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9df3055b-4668-4fe9-8e10-870f35519883",
      "metadata": {
        "id": "9df3055b-4668-4fe9-8e10-870f35519883"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}